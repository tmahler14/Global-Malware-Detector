{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Nodes Builder\n",
    "\n",
    "### Step #2.) Constructs a main file of nodes with all mapped info filled in\n",
    "### - Also cleans each node from irrelvant information and missing fields\n",
    "\n",
    "#### Tim Mahler & Atreya Misra\n",
    "#### Final Project\n",
    "#### Security EE382V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hide Warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data into a dictionary from file\n",
    "def loadDownloadedData(file_name):\n",
    "    try:\n",
    "        with open(file_name) as data_file:\n",
    "            url_list = json.load(data_file)\n",
    "        return url_list\n",
    "    except:\n",
    "        return dict()\n",
    "\n",
    "# Print pretty\n",
    "def pretty_print_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "            \n",
    "# Pretty print json\n",
    "def pretty_json(j):\n",
    "    print(json.dumps(j, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Class\n",
    "\"\"\"\n",
    "    \"node\" : {\n",
    "        \"name\" : id of Symantec file,\n",
    "        \"sha2\" : id of Symantec file SHA2,\n",
    "        \"md5\" : id of Symantec file MD5,\n",
    "        \"prob\" : probability of the node,\n",
    "        \"outcomes\" : total outcomes of malicious,\n",
    "        \"tests\" : total number of tests run\n",
    "        \"is_malicious\" : true value if malicious or not,\n",
    "        \"machine\" : <Set of machine id's where the file was found>,\n",
    "        \"predicted\" : predicted value, 1 is malicious,\n",
    "        \"ts\" : timestamp\n",
    "    },\n",
    "    \"parent\" : {\n",
    "        <node id of child> : <Edge value: Domain name>\n",
    "    },\n",
    "    \"child\" : {\n",
    "        <node id of child> : <Edge value: Domain name>\n",
    "    },\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        self.parent = {}\n",
    "        self.child = {}\n",
    "        self.machine = set()\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Node: \"+str(self.node)+\", children = \"+str(self.child)+\", parent = \"+str(self.parent)+\", machine = \"+str(self.machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the node given all of the info\n",
    "def create_node(name, ts):\n",
    "    new_node = Node({\n",
    "        \"name\" : name,\n",
    "        \"ts\" : ts\n",
    "    })\n",
    "    \n",
    "    # If node info exists, then add it. Else don't\n",
    "    if new_node.node[\"name\"] in file_labels and file_labels[new_node.node[\"name\"]]:\n",
    "        new_node.node.update(file_labels[new_node.node[\"name\"]])\n",
    "        return new_node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Mappers\n",
    "\n",
    "#### Concatenate all of the relevant mapping files for the SHA2, MDS5, porbabilities given the file id assigned by Symantec. Several levels of translation are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "158630\n",
      "SHA2\n",
      "24418232\n",
      "MD5\n",
      "24653571\n",
      "Probabilities\n",
      "Map nodes to probabilities\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Build the mapping of file to malicious vs. not malicious\n",
    "def build_file_labels():\n",
    "    \n",
    "    node_mapping = {}\n",
    "    md5_mapping = {}\n",
    "    sha2_mapping = {}\n",
    "    \n",
    "    # Open first file\n",
    "    print(\"1\")\n",
    "    with open(\"../data/labeled_IGs.csv\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        csv_lines = []\n",
    "        \n",
    "        for i, row in enumerate(readCSV):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            \n",
    "            # Else parse the mapping\n",
    "            label = row[1]\n",
    "            node_name = row[2]\n",
    "            \n",
    "            if node_name not in node_mapping:\n",
    "                node_mapping[node_name] = {}\n",
    "                node_mapping[node_name][\"malicious\"] = 1 if label == \"m\" else 0\n",
    "                \n",
    "    # 2nd files \n",
    "    print(\"2\")\n",
    "    with open(\"../data/ig_unlabel_labeled.csv\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        csv_lines = []\n",
    "        \n",
    "        for i, row in enumerate(readCSV):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            \n",
    "            # Else parse the mapping\n",
    "            label = row[1]\n",
    "            node_name = row[2]\n",
    "            \n",
    "            if node_name not in node_mapping:\n",
    "                node_mapping[node_name] = {}\n",
    "                node_mapping[node_name][\"malicious\"] = 1 if label == \"m\" else 0\n",
    "    \n",
    "    # 3rd file\n",
    "    print(\"3\")\n",
    "    with open(\"../data/updated_igs_labeled_all.csv\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        csv_lines = []\n",
    "        \n",
    "        for i, row in enumerate(readCSV):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            \n",
    "            # Else parse the mapping\n",
    "            label = row[1]\n",
    "            node_name = row[2]\n",
    "            \n",
    "            if node_name not in node_mapping:\n",
    "                node_mapping[node_name] = {}\n",
    "                node_mapping[node_name][\"malicious\"] = 1 if label == \"m\" else 0\n",
    "    \n",
    "    print (len(node_mapping))\n",
    "    \n",
    "    # SHA2 mapping\n",
    "    print (\"SHA2\")\n",
    "    with open(\"../data/filesha2.csv\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        csv_lines = []\n",
    "\n",
    "\n",
    "        for i, row in enumerate(readCSV):\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # Else parse the mapping\n",
    "            node_id = row[0]\n",
    "            sha2_id = row[1]\n",
    "            \n",
    "            if node_id not in node_mapping:\n",
    "                node_mapping[node_id] = {}\n",
    "\n",
    "            node_mapping[node_id][\"sha2\"] = sha2_id\n",
    "    \n",
    "    print (len(node_mapping))\n",
    "    \n",
    "    # MD5 mapping\n",
    "    print (\"MD5\")\n",
    "    with open(\"../data/portalfilemd5.csv\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        csv_lines = []\n",
    "\n",
    "        for i, row in enumerate(readCSV):\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # Else parse the mapping\n",
    "            node_id = row[0]\n",
    "            md5_id = row[1]\n",
    "            \n",
    "            if node_id not in node_mapping:\n",
    "                node_mapping[node_id] = {}\n",
    "\n",
    "            node_mapping[node_id][\"md5\"] = md5_id\n",
    "\n",
    "    print (len(node_mapping))\n",
    "    \n",
    "    # Map probabilities\n",
    "    print (\"Probabilities\")\n",
    "    with open(\"../data/vt_file_info_results.csv\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        csv_lines = []\n",
    "\n",
    "\n",
    "        for i, row in enumerate(readCSV):\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # Else parse the mapping\n",
    "            sha2_id = row[0]\n",
    "            md5_id = row[1]\n",
    "            outcomes = row[2]\n",
    "            total = row[3]\n",
    "            prob = round(float(outcomes)/float(total),2)\n",
    "\n",
    "            if sha2_id not in sha2_mapping:\n",
    "                sha2_mapping[sha2_id] = {\n",
    "                    \"outcomes\" : outcomes,\n",
    "                    \"total\" : total,\n",
    "                    \"prob\" : prob\n",
    "                }\n",
    "        \n",
    "            if md5_id not in md5_mapping:\n",
    "                md5_mapping[md5_id] = {\n",
    "                    \"outcomes\" : outcomes,\n",
    "                    \"total\" : total,\n",
    "                    \"prob\" : prob\n",
    "                }\n",
    "\n",
    "    # For each node, get the desired info\n",
    "    print (\"Map nodes to probabilities\")\n",
    "    for n in node_mapping.keys():\n",
    "        found = False\n",
    "        node = node_mapping[n]\n",
    "        \n",
    "        # Check the sha2\n",
    "        if \"sha2\" in node:\n",
    "            if node[\"sha2\"] in sha2_mapping:\n",
    "                found = True\n",
    "                node_mapping[n].update(sha2_mapping[node[\"sha2\"]])\n",
    "                \n",
    "        if \"md5\" in node and not found:\n",
    "            if node[\"md5\"] in md5_mapping:\n",
    "                node_mapping[n].update(md5_mapping[node[\"md5\"]])\n",
    "    \n",
    "        if \"sha2\" in node:\n",
    "            del node[\"sha2\"]\n",
    "        if \"md5\" in node:\n",
    "            del node[\"md5\"]\n",
    "    \n",
    "    print (\"Done\")\n",
    "    return node_mapping\n",
    "            \n",
    "            \n",
    "\n",
    "# Construct mapping files\n",
    "file_labels = build_file_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Mapping\n",
    "\n",
    "#### Constructs a massive key value mapping of node id to all relvant node information (id, probability, label children, parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_1_2018-12-12_16-16-46.csv at time: 2018-12-13 19:47:34.925454 with num rows = ~1,200,000\n",
      "n graphs = 0\n",
      "n graphs = 25000\n",
      "n graphs = 50000\n",
      "n graphs = 75000\n",
      "n graphs = 100000\n",
      "n graphs = 125000\n",
      "n graphs = 150000\n",
      "n graphs = 175000\n",
      "n graphs = 200000\n",
      "n graphs = 225000\n",
      "n graphs = 250000\n",
      "n graphs = 275000\n",
      "n graphs = 300000\n",
      "n graphs = 325000\n",
      "n graphs = 350000\n",
      "n graphs = 375000\n",
      "n graphs = 400000\n",
      "n graphs = 425000\n",
      "n graphs = 450000\n",
      "n graphs = 475000\n",
      "n graphs = 500000\n",
      "n graphs = 525000\n",
      "n graphs = 550000\n",
      "n graphs = 575000\n",
      "n graphs = 600000\n",
      "n graphs = 625000\n",
      "n graphs = 650000\n",
      "n graphs = 675000\n",
      "n graphs = 700000\n",
      "n graphs = 725000\n",
      "n graphs = 750000\n",
      "n graphs = 775000\n",
      "n graphs = 800000\n",
      "n graphs = 825000\n",
      "n graphs = 850000\n",
      "n graphs = 875000\n",
      "n graphs = 900000\n",
      "n graphs = 925000\n",
      "n graphs = 950000\n",
      "n graphs = 975000\n",
      "n graphs = 1000000\n",
      "n graphs = 1025000\n",
      "n graphs = 1050000\n",
      "n graphs = 1075000\n",
      "n graphs = 1100000\n",
      "n graphs = 1125000\n",
      "n graphs = 1150000\n",
      "n graphs = 1175000\n",
      "n graphs = 1200000\n",
      "n graphs = 1225000\n",
      "Done creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_1_2018-12-12_16-16-46.csv at time: 2018-12-13 19:55:59.226187\n",
      "num nodes in file = 5771380\n",
      "num good node = 571899\n",
      "num garbage node = 5199481\n",
      "num nodes no timestamp = 0\n",
      "num total nodes = 571899\n",
      "num good edges = 4729466\n",
      "num total edges = 9931201\n",
      "Creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_2_2018-12-12_16-16-46.csv at time: 2018-12-13 19:55:59.227451 with num rows = ~1,200,000\n",
      "n graphs = 0\n",
      "n graphs = 25000\n",
      "n graphs = 50000\n",
      "n graphs = 75000\n",
      "n graphs = 100000\n",
      "n graphs = 125000\n",
      "n graphs = 150000\n",
      "n graphs = 175000\n",
      "n graphs = 200000\n",
      "n graphs = 225000\n",
      "n graphs = 250000\n",
      "n graphs = 275000\n",
      "n graphs = 300000\n",
      "n graphs = 325000\n",
      "n graphs = 350000\n",
      "n graphs = 375000\n",
      "n graphs = 400000\n",
      "n graphs = 425000\n",
      "n graphs = 450000\n",
      "n graphs = 475000\n",
      "n graphs = 500000\n",
      "n graphs = 525000\n",
      "n graphs = 550000\n",
      "n graphs = 575000\n",
      "n graphs = 600000\n",
      "n graphs = 625000\n",
      "n graphs = 650000\n",
      "n graphs = 675000\n",
      "n graphs = 700000\n",
      "n graphs = 725000\n",
      "n graphs = 750000\n",
      "n graphs = 775000\n",
      "n graphs = 800000\n",
      "n graphs = 825000\n",
      "n graphs = 850000\n",
      "n graphs = 875000\n",
      "n graphs = 900000\n",
      "n graphs = 925000\n",
      "n graphs = 950000\n",
      "n graphs = 975000\n",
      "n graphs = 1000000\n",
      "n graphs = 1025000\n",
      "n graphs = 1050000\n",
      "n graphs = 1075000\n",
      "n graphs = 1100000\n",
      "n graphs = 1125000\n",
      "n graphs = 1150000\n",
      "n graphs = 1175000\n",
      "n graphs = 1200000\n",
      "n graphs = 1225000\n",
      "n graphs = 1250000\n",
      "Done creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_2_2018-12-12_16-16-46.csv at time: 2018-12-13 20:03:43.745503\n",
      "num nodes in file = 5117292\n",
      "num good node = 278653\n",
      "num garbage node = 4838639\n",
      "num nodes no timestamp = 0\n",
      "num total nodes = 850552\n",
      "num good edges = 4760007\n",
      "num total edges = 9600373\n",
      "Creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_3_2018-12-12_16-16-46.csv at time: 2018-12-13 20:03:43.746480 with num rows = ~1,200,000\n",
      "n graphs = 0\n",
      "n graphs = 25000\n",
      "n graphs = 50000\n",
      "n graphs = 75000\n",
      "n graphs = 100000\n",
      "n graphs = 125000\n",
      "n graphs = 150000\n",
      "n graphs = 175000\n",
      "n graphs = 200000\n",
      "n graphs = 225000\n",
      "n graphs = 250000\n",
      "n graphs = 275000\n",
      "n graphs = 300000\n",
      "n graphs = 325000\n",
      "n graphs = 350000\n",
      "n graphs = 375000\n",
      "n graphs = 400000\n",
      "n graphs = 425000\n",
      "n graphs = 450000\n",
      "n graphs = 475000\n",
      "n graphs = 500000\n",
      "n graphs = 525000\n",
      "n graphs = 550000\n",
      "n graphs = 575000\n",
      "n graphs = 600000\n",
      "n graphs = 625000\n",
      "n graphs = 650000\n",
      "n graphs = 675000\n",
      "n graphs = 700000\n",
      "n graphs = 725000\n",
      "n graphs = 750000\n",
      "n graphs = 775000\n",
      "n graphs = 800000\n",
      "n graphs = 825000\n",
      "n graphs = 850000\n",
      "n graphs = 875000\n",
      "n graphs = 900000\n",
      "n graphs = 925000\n",
      "n graphs = 950000\n",
      "n graphs = 975000\n",
      "n graphs = 1000000\n",
      "n graphs = 1025000\n",
      "n graphs = 1050000\n",
      "n graphs = 1075000\n",
      "n graphs = 1100000\n",
      "n graphs = 1125000\n",
      "n graphs = 1150000\n",
      "n graphs = 1175000\n",
      "Done creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_3_2018-12-12_16-16-46.csv at time: 2018-12-13 20:10:53.879342\n",
      "num nodes in file = 4471100\n",
      "num good node = 293717\n",
      "num garbage node = 4177383\n",
      "num nodes no timestamp = 0\n",
      "num total nodes = 1144269\n",
      "num good edges = 5038267\n",
      "num total edges = 9217627\n",
      "Creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_4_2018-12-12_16-16-46.csv at time: 2018-12-13 20:10:53.879673 with num rows = ~1,200,000\n",
      "n graphs = 0\n",
      "n graphs = 25000\n",
      "n graphs = 50000\n",
      "n graphs = 75000\n",
      "n graphs = 100000\n",
      "n graphs = 125000\n",
      "n graphs = 150000\n",
      "n graphs = 175000\n",
      "n graphs = 200000\n",
      "n graphs = 225000\n",
      "n graphs = 250000\n",
      "n graphs = 275000\n",
      "n graphs = 300000\n",
      "n graphs = 325000\n",
      "n graphs = 350000\n",
      "n graphs = 375000\n",
      "n graphs = 400000\n",
      "n graphs = 425000\n",
      "n graphs = 450000\n",
      "n graphs = 475000\n",
      "n graphs = 500000\n",
      "n graphs = 525000\n",
      "n graphs = 550000\n",
      "n graphs = 575000\n",
      "n graphs = 600000\n",
      "n graphs = 625000\n",
      "n graphs = 650000\n",
      "n graphs = 675000\n",
      "n graphs = 700000\n",
      "n graphs = 725000\n",
      "n graphs = 750000\n",
      "n graphs = 775000\n",
      "n graphs = 800000\n",
      "n graphs = 825000\n",
      "n graphs = 850000\n",
      "n graphs = 875000\n",
      "n graphs = 900000\n",
      "n graphs = 925000\n",
      "n graphs = 950000\n",
      "n graphs = 975000\n",
      "n graphs = 1000000\n",
      "n graphs = 1025000\n",
      "n graphs = 1050000\n",
      "n graphs = 1075000\n",
      "n graphs = 1100000\n",
      "n graphs = 1125000\n",
      "n graphs = 1150000\n",
      "n graphs = 1175000\n",
      "n graphs = 1200000\n",
      "n graphs = 1225000\n",
      "n graphs = 1250000\n",
      "Done creating nodes for file: ../data/constructed_graphs/constructed_downloader_graph_4_2018-12-12_16-16-46.csv at time: 2018-12-13 20:16:33.532346\n",
      "num nodes in file = 3494392\n",
      "num good node = 291673\n",
      "num garbage node = 3202719\n",
      "num nodes no timestamp = 0\n",
      "num total nodes = 1435942\n",
      "num good edges = 4437837\n",
      "num total edges = 7641429\n"
     ]
    }
   ],
   "source": [
    "# Constructs graph\n",
    "# - Creates edges between file nodes if the edge has a suspicous domain associated with it\n",
    "FILE_NAME = \"../data/constructed_graphs/constructed_downloader_graph_%d_2018-12-12_16-16-46.csv\"\n",
    "all_nodes = {}\n",
    "url_mapping = {}\n",
    "url_counter = 0\n",
    "suspicious_domains = pd.read_csv('../data/suspicious_domains.csv', error_bad_lines=False, header=None)\n",
    "suspicious = set(suspicious_domains[0])\n",
    "urls = []\n",
    "\n",
    "# Map URL to identifier\n",
    "def map_url(url):\n",
    "    global url_mapping, url_counter\n",
    "    \n",
    "    if url in url_mapping:\n",
    "        return url_mapping[url]\n",
    "    else:\n",
    "        url_counter += 1\n",
    "        url_mapping[url] = url_counter\n",
    "        return url_counter\n",
    "    \n",
    "\n",
    "# Constructs the main graph file given the nodes\n",
    "def constructGraphs(downloader_file, suspicious):\n",
    "    global all_nodes, url_mapping, url_counter\n",
    "    onlySuspicious = False\n",
    "    num_nodes = 0\n",
    "    num_garbage_nodes = 0\n",
    "    num_good_nodes = 0\n",
    "    num_edges = 0\n",
    "    num_good_edges = 0\n",
    "    empty_ts = 0\n",
    "    \n",
    "    with open(downloader_file) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        print (\"Creating nodes for file: \"+downloader_file+\" at time: \"+str(datetime.datetime.now())+\" with num rows = ~1,200,000\")\n",
    "        csv_lines = []\n",
    "        \n",
    "        # For each graph, construct nodes and edges\n",
    "        for i, row in enumerate(readCSV):\n",
    "            machine_id = row[0]\n",
    "            graph = json.loads(json.dumps(ast.literal_eval(row[2])))\n",
    "            \n",
    "            # Print state\n",
    "            if i % 25000 == 0:\n",
    "                print(\"n graphs = \"+str(i))\n",
    "            \n",
    "            # Limit the size of the graphs\n",
    "#             if len(graph[\"nodes\"]) < 20:\n",
    "#                 continue\n",
    "            \n",
    "            #print (graph)\n",
    "            \n",
    "            # Loop through nodes\n",
    "            for n in graph['nodes']:\n",
    "                # If not doesn't exist, then add it\n",
    "                if n['name'] not in all_nodes:\n",
    "                    num_nodes += 1\n",
    "                    \n",
    "                    # Fill in blank timestamp\n",
    "                    if n['ts'] == \"N/A\":\n",
    "                        temp_ts = \"\"\n",
    "                        for n2 in graph['nodes']:\n",
    "                            if n2['ts'] != \"N/A\":\n",
    "                                temp_ts = n2['ts']\n",
    "                                break\n",
    "                        n['ts'] = temp_ts\n",
    "                        if not n['ts']:\n",
    "                            empty_ts += 1 # Count number of missing timestamps\n",
    "                    \n",
    "                    new_node = create_node(n['name'], n['ts'])\n",
    "                    \n",
    "                    if new_node:\n",
    "                        num_good_nodes += 1\n",
    "                        all_nodes[n['name']] = new_node\n",
    "                    else:\n",
    "                        num_garbage_nodes += 1\n",
    "                    \n",
    "                \n",
    "                # Add machine id to the node \n",
    "                #print (all_nodes[n['name']])\n",
    "                \n",
    "                if n['name'] in all_nodes:\n",
    "                    all_nodes[n['name']].machine.add(machine_id)\n",
    "                \n",
    "            # Loop through edges\n",
    "            for e in graph['edges']:\n",
    "                num_edges += 1\n",
    "                \n",
    "                # Add url to mapping\n",
    "                \n",
    "                u = e['url'][7:] # Remove the http prefix\n",
    "                \n",
    "                url_id = map_url(u) # Returns the identifier for the url\n",
    "\n",
    "                # Only add edges that exist\n",
    "                if e['target'] in all_nodes and e['source'] in all_nodes:\n",
    "                    num_good_edges += 1\n",
    "                    if onlySuspicious:\n",
    "                        if e['url'][7:] in suspicious:\n",
    "                            all_nodes[e['source']].child[e['target']] = url_id\n",
    "                            all_nodes[e['target']].parent[e['source']] = url_id\n",
    "\n",
    "                    # Else add all of the edges\n",
    "                    else:\n",
    "                        all_nodes[e['source']].child[e['target']] = url_id\n",
    "                        all_nodes[e['target']].parent[e['source']] = url_id\n",
    "                        \n",
    "            \n",
    "    \n",
    "    print (\"Done creating nodes for file: \"+downloader_file+\" at time: \"+str(datetime.datetime.now()))\n",
    "    print (\"num nodes in file = \"+str(num_nodes))\n",
    "    print (\"num good node = \"+str(num_good_nodes))\n",
    "    print (\"num garbage node = \"+str(num_garbage_nodes))\n",
    "    print (\"num nodes no timestamp = \"+str(empty_ts))\n",
    "    print (\"num total nodes = \"+str(len(all_nodes)))\n",
    "    print (\"num good edges = \"+str(num_good_edges))\n",
    "    print (\"num total edges = \"+str(num_edges))\n",
    "    return\n",
    "\n",
    "\n",
    "# For each constructed graph file, add the node\n",
    "constructGraphs(FILE_NAME % 1, suspicious)\n",
    "constructGraphs(FILE_NAME % 2, suspicious)\n",
    "constructGraphs(FILE_NAME % 3, suspicious)\n",
    "constructGraphs(FILE_NAME % 4, suspicious)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1435942\n",
      "4\n",
      "358985\n",
      "Creating chunk file: ../data/mapped_nodes/mapped_nodes_0_2018-12-13_20-37-14.csv at time: 2018-12-13 20:37:14.250225\n",
      "first = 0\n",
      "second = 358985\n",
      "\n",
      "Creating chunk file: ../data/mapped_nodes/mapped_nodes_1_2018-12-13_20-37-14.csv at time: 2018-12-13 20:37:27.271626\n",
      "first = 358985\n",
      "second = 717970\n",
      "\n",
      "Creating chunk file: ../data/mapped_nodes/mapped_nodes_2_2018-12-13_20-37-14.csv at time: 2018-12-13 20:37:34.973693\n",
      "first = 717970\n",
      "second = 1076955\n",
      "\n",
      "Creating chunk file: ../data/mapped_nodes/mapped_nodes_3_2018-12-13_20-37-14.csv at time: 2018-12-13 20:37:41.713896\n",
      "first = 1076955\n",
      "second = 1435940\n",
      "\n",
      "Saving the URL mapping\n",
      "Size of the url list = 253450\n",
      "Total saved nodes = 1435940\n"
     ]
    }
   ],
   "source": [
    "keys = list(all_nodes.keys())\n",
    "num_keys = len(keys)\n",
    "num_chunks = 4\n",
    "chunk_size = int(num_keys / num_chunks)\n",
    "total_nodes_saved = 0\n",
    "\n",
    "directory = \"../data/mapped_nodes\"\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "NODES_FILE_NAME = directory + \"/mapped_nodes_%d_\"+time.strftime(\"%Y-%m-%d_%H-%M-%S\")+\".csv\"\n",
    "MACHINE_FILE_NAME = directory + \"/mapped_nodes_%d_machines_\"+time.strftime(\"%Y-%m-%d_%H-%M-%S\")+\".csv\"\n",
    "URL_FILE_NAME = directory + \"/mapped_urls_\"+time.strftime(\"%Y-%m-%d_%H-%M-%S\")+\".csv\"\n",
    "\n",
    "# print (keys[2])\n",
    "# print (all_nodes[keys[2]])\n",
    "\n",
    "print (num_keys)\n",
    "print (num_chunks)\n",
    "print (chunk_size)\n",
    "\n",
    "# Write to the csv\n",
    "def write_to_csv(file_name, data):\n",
    "    # Write new line to the csv file\n",
    "    with open(file_name, \"a\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        \n",
    "        for line in data:\n",
    "            writer.writerow(line)\n",
    "            \n",
    "# Save the url mapping\n",
    "def save_url_mapping(file_name):\n",
    "    print (\"Saving the URL mapping\")\n",
    "    \n",
    "    url_data = []\n",
    "    \n",
    "    print (\"Size of the url list = \"+str(len(list(url_mapping.keys()))))\n",
    "    \n",
    "    # Make the values the keys\n",
    "    for k, v in url_mapping.items():\n",
    "        url_data.append([k, v])\n",
    "    \n",
    "    # Write to csv\n",
    "    write_to_csv(file_name, url_data)\n",
    "\n",
    "# Def save node chunks\n",
    "def save_node_chunk(chunk_num, node_file_name, machine_file_name):\n",
    "    global total_nodes_saved\n",
    "    print (\"Creating chunk file: \"+node_file_name+\" at time: \"+str(datetime.datetime.now()))\n",
    "    \n",
    "    first_range = chunk_num * chunk_size\n",
    "    second_range = ((chunk_num + 1) * chunk_size) if chunk_num < num_chunks else num_keys\n",
    "    \n",
    "    print (\"first = \"+str(first_range))\n",
    "    print (\"second = \"+str(second_range))\n",
    "    print (\"\")\n",
    "    \n",
    "\n",
    "    # For each range in key, save to the file\n",
    "    csv_data = []\n",
    "    machine_data = []\n",
    "    \n",
    "    for k in keys[first_range : second_range]:\n",
    "        n = all_nodes[k]\n",
    "        node_data = {\n",
    "            \"node\" : n.node,\n",
    "            \"child\" : n.child,\n",
    "            \"parent\" : n.parent\n",
    "        }\n",
    "        machine_data = {\n",
    "            \"machine\" : n.machine\n",
    "        }\n",
    "#         print(node_data)\n",
    "# #         print ([n.node[\"name\"], node_data])\n",
    "#         break\n",
    "        total_nodes_saved += 1\n",
    "        csv_data.append([node_data])\n",
    "        #machine_data.append([n.node[\"name\"], machine_data])\n",
    "        \n",
    "    # Write to csv\n",
    "    write_to_csv(node_file_name, csv_data)\n",
    "    #write_to_csv(machine_file_name, machine_data)\n",
    "\n",
    "\n",
    "# Run through and build csv for each chunk\n",
    "for c in range(0, num_chunks):\n",
    "    save_node_chunk(c, NODES_FILE_NAME % c, MACHINE_FILE_NAME % c)\n",
    "    \n",
    "# Run through and save in chunks\n",
    "save_url_mapping(URL_FILE_NAME)\n",
    "\n",
    "print (\"Total saved nodes = \"+str(total_nodes_saved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
